{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EEVE Model Basic Testing\n",
    "\n",
    "Code to test the baseline functionality of the EEVE model by yanolja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kimjaehyun/anaconda3/envs/nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import modules \n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model name for loading from the transformers library\n",
    "\n",
    "# # Yanolja/EEVE\n",
    "# model_name = \"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\"\n",
    "# tokenizer_name = \"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\"\n",
    "\n",
    "#  maywell/Synatra-V0.1-7B\n",
    "model_name = \"maywell/Synatra-V0.1-7B\"\n",
    "tokenizer_name = \"maywell/Synatra-V0.1-7B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [21:08<00:00, 634.06s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.60s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16, # half-precision to speed up inference\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,\n",
    "                                          device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set text\n",
    "\n",
    "prompt_template = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\\nHuman: {prompt}\\nAssistant:\\n\"\n",
    "text = \"한국의 수도는 어디인가요? 아래 선택지 중 골라주세요.\\n\\n(A) 경성\\n(B) 부산\\n(C) 평양\\n(D) 서울\\n(E) 전주\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
      "Human: 한국의 수도는 어디인가요? 아래 선택지 중 골라주세요.\n",
      "\n",
      "(A) 경성\n",
      "(B) 부산\n",
      "(C) 평양\n",
      "(D) 서울\n",
      "(E) 전주\n",
      "Assistant:\n",
      "\n",
      "AI:  한국의 수도는 (A) 경성입니다. 경성은 서울특별시를 의미하며, 이 지역은 국가 정부 기관, 대통령 및 대한민국 국회의 본부가 위치해 있습니다. 다른 옵션은 대도시나 지방 자치단체의 수도가 아닙니다.</s>\n"
     ]
    }
   ],
   "source": [
    "# Process through model and obtain model output\n",
    "with torch.no_grad():\n",
    "    model_inputs = tokenizer(prompt_template.format(prompt=text), return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    model.generation_config.pad_token_id = model.generation_config.eos_token_id  # To prevent early ending of sentence\n",
    "    outputs = model.generate(**model_inputs, max_new_tokens=256)\n",
    "    # outputs = model.generate(**model_inputs, pad_token_id=tokenizer.eos_token_id)\n",
    "output_text = tokenizer.batch_decode(outputs, skip_special_token=True)[0]\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kimjaehyun/anaconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:615: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " AI :   한 국 의  수 도 는 ( A )  경 성 입 니 다 .  경 성 은  서 울 특 별 시 를  의 미 하 며 ,  이  지 역 은  국 가  정 부  기 관 ,  대 통 령  및  대 한 민 국  국 회 의  본 부 가  위 치 해  있 습 니 다 .  다 른  옵 션 은  대 도 시 나  지 방  자 치 단 체 의  수 도 가  아 � � � 니 다 .  "
     ]
    }
   ],
   "source": [
    "# Generate tokens one by one and print them\n",
    "with torch.no_grad():\n",
    "    model_inputs = tokenizer(prompt_template.format(prompt=text), return_tensors=\"pt\").to(\"cuda\")\n",
    "    output_ids = model_inputs['input_ids']\n",
    "    while True:\n",
    "        outputs = model.generate(output_ids, max_new_tokens=1, early_stopping=True)\n",
    "        new_token_id = outputs[0, -1:]\n",
    "        output_ids = torch.cat([output_ids, new_token_id.unsqueeze(0)], dim=-1)\n",
    "        \n",
    "        # Decode and print the latest token\n",
    "        new_token = tokenizer.decode(new_token_id, skip_special_tokens=True)\n",
    "        print(new_token, end=' ', flush=True)\n",
    "        \n",
    "        # Stop if the generated token is eos_token\n",
    "        if new_token_id.item() == tokenizer.eos_token_id:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
