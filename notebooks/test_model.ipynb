{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baisc Model Testing\n",
    "\n",
    "Code to test the baseline functionality of various models provided by the transformers library. \n",
    "\n",
    "This includes llama-3-Korean-Bllossom-8B, the EEVE model by yanolja, the Synatra model, and others.\n",
    "\n",
    "This notebook is to test the basic capabilities and functionalities of the model, and doesn't not involve any specialized methods such as training, fine-tuning, RAG, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kimjaehyun/anaconda3/envs/nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import modules \n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model name for loading from the transformers library\n",
    "\n",
    "# Yanolja/EEVE\n",
    "model_name = \"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\"\n",
    "tokenizer_name = \"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\"\n",
    "\n",
    "# # maywell/Synatra-V0.1-7B, maywell/Synatra-7B-Instruct-v0.2\n",
    "# model_name = \"maywell/Synatra-7B-Instruct-v0.2\"\n",
    "# tokenizer_name = \"maywell/Synatra-7B-Instruct-v0.2\"\n",
    "\n",
    "# # openchat/openchat_3.5\n",
    "# model_name = \"openchat/openchat_3.5\"\n",
    "# tokenizer_name = \"openchat/openchat_3.5\"\n",
    "\n",
    "# # monologg/kobert\n",
    "# model_name = \"monologg/kobert\"\n",
    "# tokenizer_name = \"monologg/kobert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:10<00:00,  2.03s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16, # half-precision to speed up inference\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,\n",
    "                                          device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set text and prompt template\n",
    "\n",
    "# # Yanolja/EEVE prompt\n",
    "prompt_template = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\\nHuman: {prompt}\\nAssistant:\\n\"\n",
    "\n",
    "# # maywell/Synatra prompt\n",
    "# prompt_template = \"<s>[INST] {prompt}. [/INST]\"\n",
    "\n",
    "# # OpenChat prompt\n",
    "# prompt_template = \"GPT4 Correct User: Hello<|end_ofturn|>GPT4 Correct Assistant: Hi<|end_of_turn|>GPT4 Correct User: {prompt}<|end_of_turn|>GPT4 Correct Assistant:\\n\"\n",
    "\n",
    "# text = \"한국의 수도는 어디인가요? 아래 선택지 중 골라주세요.\\n\\n(A) 경성\\n(B) 부산\\n(C) 평양\\n(D) 서울\\n(E) 전주\"\n",
    "text = \"\"\"당신은 판사입니다. 다음과 같은 청구취지와 항소취지를 보고, 편결요지를 내십시오.\n",
    "\n",
    "1. 청구취지\n",
    "\n",
    "가. 본소\n",
    "\n",
    "원고(반소피고, 이하 원고라 한다)와 피고(반소원고, 이하 피고라 한다)는 이혼한다.\n",
    "\n",
    "나. 반소\n",
    "\n",
    "원고의 본소 청구가 인용되는 것을 조건으로, 원고는 피고에게, 위자료로 1억 원 및 이에 대하여 반소장 송달 다음 날부터 제1심판결 선고일까지는 연 5%의, 그 다음 날부터 다 갚는 날까지는 연 20%의 각 비율에 의한 금원을, 재산분할로 514,800,000원 및 이에 대하여 이 판결 확정일 다음 날부터 다 갚는 날까지 연 5%의 비율에 의한 금원을 각 지급하라.\n",
    "\n",
    "2. 항소취지\n",
    "\n",
    "제1심판결 중 본소 부분을 취소하고, 원고의 본소 청구를 기각한다. 원고의 본소 청구가 인용되는 것을 조건으로, 제1심판결의 반소 부분 중 아래에서 추가로 지급을 명하는 부분에 해당하는 피고 패소 부분을 취소한다. 원고는 피고에게, 위자료로 20,000,000원 및 이에 대하여 이 사건 예비적 반소장 송달 다음 날부터 당심판결 선고일까지는 연 5%의, 그 다음 날부터 다 갚는 날까지는 연 20%의 각 비율에 의한 금원을, 재산분할로 499,800,000원 및 이에 대하여 이 판결 확정일 다음 날부터 다 갚는 날까지 연 5%의 비율에 의한 금원을, 각 지급하라.\n",
    "\n",
    "3. 부대항소취지\n",
    "\n",
    "제1심판결의 반소 중 원고에 대하여 위자료로 50,000,000원을 초과하여 지급을 명한 원고 패소 부분과 재산분할에 관한 원고 패소 부분을 취소하고, 그 취소 부분에 해당하는 피고의 반소 청구를 기각한다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
      "Human: 당신은 판사입니다. 다음과 같은 청구취지와 항소취지를 보고, 편결요지를 내십시오.\n",
      "\n",
      "1. 청구취지\n",
      "\n",
      "가. 본소\n",
      "\n",
      "원고(반소피고, 이하 원고라 한다)와 피고(반소원고, 이하 피고라 한다)는 이혼한다.\n",
      "\n",
      "나. 반소\n",
      "\n",
      "원고의 본소 청구가 인용되는 것을 조건으로, 원고는 피고에게, 위자료로 1억 원 및 이에 대하여 반소장 송달 다음 날부터 제1심판결 선고일까지는 연 5%의, 그 다음 날부터 다 갚는 날까지는 연 20%의 각 비율에 의한 금원을, 재산분할로 514,800,000원 및 이에 대하여 이 판결 확정일 다음 날부터 다 갚는 날까지 연 5%의 비율에 의한 금원을 각 지급하라.\n",
      "\n",
      "2. 항소취지\n",
      "\n",
      "제1심판결 중 본소 부분을 취소하고, 원고의 본소 청구를 기각한다. 원고의 본소 청구가 인용되는 것을 조건으로, 제1심판결의 반소 부분 중 아래에서 추가로 지급을 명하는 부분에 해당하는 피고 패소 부분을 취소한다. 원고는 피고에게, 위자료로 20,000,000원 및 이에 대하여 이 사건 예비적 반소장 송달 다음 날부터 당심판결 선고일까지는 연 5%의, 그 다음 날부터 다 갚는 날까지는 연 20%의 각 비율에 의한 금원을, 재산분할로 499,800,000원 및 이에 대하여 이 판결 확정일 다음 날부터 다 갚는 날까지 연 5%의 비율에 의한 금원을, 각 지급하라.\n",
      "\n",
      "3. 부대항소취지\n",
      "\n",
      "제1심판결의 반소 중 원고에 대하여 위자료로 50,000,000원을 초과하여 지급을 명한 원고 패소 부분과 재산분할에 관한 원고 패소 부분을 취소하고, 그 취소 부분에 해당하는 피고의 반소 청구를 기각한다.\n",
      "\n",
      "Assistant:\n",
      "판사로서, 청구취지와 항소에 근거하여 다음과 같은 편결요지를 내리겠습니다:\n",
      "\n",
      "본소:\n",
      "\n",
      "원고와 피고 사이의 이혼을 인용합니다.\n",
      "\n",
      "반소:\n",
      "\n",
      "1. 원고의 본소가 인용되는 것을 조건부로, 피고는 원고에게 위자료로 40,00,0원 및 이에 대한 이 사건 예비적 항소장 송달일 다음 날부터 제2심판결 선고를 선고하는 날까지 연 10%의, 그 이후부터 다 갚을 때까지 연 20 %의 각 비율로 계산한 금원을 지급해야 합니다.\n",
      "2. 원고의 본사가 인용되는 것을 조건부, 피고는 원고의 재산분할 청구를 기각합니다.\n",
      "\n",
      "부대항소취지에 대하여:\n",
      "\n",
      "1심판결에서 원고에 대한 위자료 지급을 50,400,0원으로 초과하여 명령한 원고 패소의 부분을 취소하고, 취소된 부분에 해당하는 피고의 청구를 기각하며, 대신에 피고는 원고에 대하여 40,40,0원의 위자료를 지급해야 합니다. 이는 이 사건 예비적항소장 송달 이후부터 제2심 판결 선고일까지 연 15%의, 이후부터 다 빚을 갚을 때까지 20%씩 계산한 비율로 지급해야 합니다.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# Process through model and obtain model output\n",
    "with torch.no_grad():\n",
    "    model_inputs = tokenizer(prompt_template.format(prompt=text), return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    model.generation_config.pad_token_id = model.generation_config.eos_token_id  # To prevent early ending of sentence\n",
    "    outputs = model.generate(**model_inputs, max_new_tokens=2048, no_repeat_ngram_size=5)\n",
    "    # outputs = model.generate(**model_inputs, pad_token_id=tokenizer.eos_token_id)\n",
    "output_text = tokenizer.batch_decode(outputs, skip_special_token=True)[0]\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Nearly) Identical operation as except tokens are generated and printed one by one\n",
    "with torch.no_grad():\n",
    "    model_inputs = tokenizer(prompt_template.format(prompt=text), return_tensors=\"pt\")#.to(\"cuda\")\n",
    "    output_ids = model_inputs['input_ids']\n",
    "    while True:\n",
    "        outputs = model.generate(output_ids, max_new_tokens=1, early_stopping=True)\n",
    "        new_token_id = outputs[0, -1:]\n",
    "        output_ids = torch.cat([output_ids, new_token_id.unsqueeze(0)], dim=-1)\n",
    "        \n",
    "        # Decode and print the latest token\n",
    "        new_token = tokenizer.decode(new_token_id, skip_special_tokens=True)\n",
    "        print(new_token, end=' ', flush=True)\n",
    "        \n",
    "        # Stop if the generated token is eos_token\n",
    "        if new_token_id.item() == tokenizer.eos_token_id:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets try to test the Blossom 8B model, as provided in the HuggingFace library:\n",
    "\n",
    "https://huggingface.co/MLP-KTLim/llama-3-Korean-Bllossom-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 판결문\n",
      "\n",
      "**재판장**  \n",
      "[판사 이름]\n",
      "\n",
      "**[일자]**  \n",
      "[판결번호]  \n",
      "[소송번호]  \n",
      "[소송이름]  \n",
      "[소송구성]\n",
      "\n",
      "**판결문**\n",
      "\n",
      "1. 본소 청구는 기각한다.\n",
      "2. 반소 청구는 기각한다.\n",
      "\n",
      "**이유**\n",
      "1. 본소 청구에 대한 판결:\n",
      "제1심판결의 본소 부분은 원고가 피고에게 위자료로 1억 원 및 이에 대하여 반소장 송달 다음 날부터 제1심판결 선고일까지는 연 5%의, 그 다음 날부터 다 갚는 날까지는 연 20%의 각 비율에 의한 금원을, 재산분할로 514,800,000원 및 이에 대하여 이 판결 확정일 다음 날부터 다 갚는 날까지 연 5%의 비율에 의한 금원을 각 지급하라는 조건으로 인용되는 것을 조건으로, 원고는 피고에게 위와 같은 금원을 지급하라는 명령을 내렸다. 제1심판결의 본소 부분은 위의 조건을 충족하는 경우에만 원고의 청구를 인용하는 것이므로, 원고가 위 조건을 충족하지 못한 상태에서 본소 청구를 인용할 수 없기 때문에 본소 청구는 기각한다.\n",
      "\n",
      "2. 반소 청구에 대한 판결:\n",
      "제1심판결의 반소 부분은 원고에게 위자료로 1억 원 및 이에 대하여 반소장 송달 다음 날부터 제1심판결 선고일까지는 연 5%의, 그 다음 날부터 다 갚는 날까지는 연 20%의 각 비율에 의한 금원을, 재산분할로 514,800,000원 및 이에 대하여 이 판결 확정일 다음 날부터 다 갚는 날까지 연 5%의 비율에 의한 금원을 각 지급하라는 명령을 내렸다. 원고가 위 조건을 충족하지 못한 상태에서 반소 청구를 인용할 수 없기 때문에 반소 청구는 기각한다.\n",
      "\n",
      "**결론**\n",
      "이와 같은 이유로, 본소 청구는 기각하고, 반소 청구는 기각한다.\n",
      "\n",
      "**판사 서명**\n",
      "[판사 이름]  \n",
      "[판사 서명]  \n",
      "[일자]\n"
     ]
    }
   ],
   "source": [
    "# Using transformers pipeline for Blossom 8B\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n",
    "\n",
    "model = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model.model.eval()\n",
    "\n",
    "PROMPT = '''You are a helpful AI assistant. Please answer the user's questions kindly. 당신은 유능한 AI 어시스턴트 입니다. 사용자의 질문에 대해 친절하게 답변해주세요.'''\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": f\"{PROMPT}\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{text}\"}\n",
    "    ]\n",
    "\n",
    "prompt = model.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    model.tokenizer.eos_token_id,\n",
    "    model.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model(\n",
    "    prompt,\n",
    "    max_new_tokens=2048,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
