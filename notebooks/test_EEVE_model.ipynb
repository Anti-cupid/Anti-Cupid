{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EEVE Model Basic Testing\n",
    "\n",
    "Code to test the baseline functionality of the EEVE model by yanolja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kimjaehyun/anaconda3/envs/nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import modules \n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:46<00:00,  9.29s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16, # half-precision to speed up inference\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\",\n",
    "                                          device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set text\n",
    "\n",
    "prompt_template = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\\nHuman: {prompt}\\nAssistant:\\n\"\n",
    "text = \"한국의 수도는 어디인가요? 아래 선택지 중 골라주세요.\\n\\n(A) 경성\\n(B) 부산\\n(C) 평양\\n(D) 서울\\n(E) 전주\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
      "Human: 한국의 수도는 어디인가요? 아래 선택지 중 골라주세요.\n",
      "\n",
      "(A) 경성\n",
      "(B) 부산\n",
      "(C) 평양\n",
      "(D) 서울\n",
      "(E) 전주\n",
      "Assistant:\n",
      "(D) 서울이 한국의 수도입니다. 서울은 나라의 북동부에 위치해 있으며, 정치, 경제, 문화의 중심지입니다. 약 1,000만 명이 넘는 인구를 가진 세계에서 가장 큰 도시 중 하나입니다. 서울은 높은 빌딩, 현대적인 인프라, 활기찬 문화 장면으로 유명합니다. 또한, 많은 역사적 명소와 박물관이 있어 방문객들에게 풍부한 문화 체험을 제공합니다.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# Process through model and obtain model output\n",
    "with torch.no_grad():\n",
    "    model_inputs = tokenizer(prompt_template.format(prompt=text), return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    model.generation_config.pad_token_id = model.generation_config.eos_token_id  # To prevent early ending of sentence\n",
    "    outputs = model.generate(**model_inputs, max_new_tokens=256)\n",
    "    # outputs = model.generate(**model_inputs, pad_token_id=tokenizer.eos_token_id)\n",
    "output_text = tokenizer.batch_decode(outputs, skip_special_token=True)[0]\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kimjaehyun/anaconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:615: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( D ) 서울 이 한국의 수도 입니다 . 서울 은 나라의 북 동 부에 위치 해 있으며 , 정치 , 경제 , 문화 의 중심 지 입니다 . 약  1 , 0 0 0 만 명이  넘 는 인구 를 가진 세계에서 가장 큰 도시 중 하나입니다 . 서울 은  높 은 빌 딩 , 현대 적인 인 프 라 , 활 기 찬 문화 장면 으로 유명 합니다 . 또한 ,  많 은 역사적 명 소 와 박 물 관이 있어 방문 객 들에게 풍부한 문화 체 험 을 제공합니다 .  "
     ]
    }
   ],
   "source": [
    "# Generate tokens one by one and print them\n",
    "with torch.no_grad():\n",
    "    model_inputs = tokenizer(prompt_template.format(prompt=text), return_tensors=\"pt\").to(\"cuda\")\n",
    "    output_ids = model_inputs['input_ids']\n",
    "    while True:\n",
    "        outputs = model.generate(output_ids, max_new_tokens=1, early_stopping=True)\n",
    "        new_token_id = outputs[0, -1:]\n",
    "        output_ids = torch.cat([output_ids, new_token_id.unsqueeze(0)], dim=-1)\n",
    "        \n",
    "        # Decode and print the latest token\n",
    "        new_token = tokenizer.decode(new_token_id, skip_special_tokens=True)\n",
    "        print(new_token, end=' ', flush=True)\n",
    "        \n",
    "        # Stop if the generated token is eos_token\n",
    "        if new_token_id.item() == tokenizer.eos_token_id:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
